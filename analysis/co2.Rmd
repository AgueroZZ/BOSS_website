---
title: "Example 1: Decomposition of CO2 Variation"
author: "Ziang Zhang"
date: "2025-04-18"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Read in the data

In this example, we apply the BOSS algorithm to analyze the atmospheric carbon dioxide (CO2) concentration data, which were collected from an observatory in Hawaii monthly before May 1974, and weekly afterward. 
In total, this dataset consists of $n = 2267$ observations.

```{r}
library(BayesGP)
library(tidyverse)
library(npreg)
function_path <- "./code"
output_path <- "./output/co2"
data_path <- "./data/co2"
source(paste0(function_path, "/00_BOSS.R"))
```


```{r}
### Read in the full data:
co2s = read.table(paste0(data_path, "/daily_flask_co2_mlo.csv"), header = FALSE, sep = ",",
                  skip = 69, stringsAsFactors = FALSE, col.names = c("day",
                                                                     "time", "junk1", "junk2", "Nflasks", "quality",
                                                                     "co2"))
co2s$date = strptime(paste(co2s$day, co2s$time), format = "%Y-%m-%d %H:%M",
                     tz = "UTC")
co2s$day = as.Date(co2s$date)
timeOrigin = as.Date("1960/03/30")
co2s$timeYears = round(as.numeric(co2s$day - timeOrigin)/365.25,
                         3)
co2s$dayInt = as.integer(co2s$day)
allDays = seq(from = min(co2s$day), to = max(co2s$day),
              by = "7 day")
observed_dataset <- co2s %>% filter(!is.na(co2s$co2)) %>% dplyr::select(c("co2", "timeYears"))
observed_dataset$quality <- ifelse(co2s$quality > 0, 1, 0)
observed_dataset <- observed_dataset %>% filter(quality == 0)
### Create the covariate for trend and the 1-year seasonality 
observed_dataset$t1 <- observed_dataset$timeYears
observed_dataset$t2 <- observed_dataset$timeYears
observed_dataset$t3 <- observed_dataset$timeYears
observed_dataset$t4 <- observed_dataset$timeYears
```

Motivated by the literature, the hierarchical model has the following structure:
\begin{equation}
    \begin{aligned}
        y_i &= g_{tr}(x_i) + g_{1}(x_i) + g_{\frac{1}{2}}(x_i) + g_{\alpha}(x_i) + e_i,\\
        g_{1} &\sim \text{sGP}_1(\sigma_{s}),\  g_{\frac{1}{2}} \sim \text{sGP}_{\frac{1}{2}}(\sigma_{s}) \\
        g_{\alpha} &\sim \text{sGP}_{\alpha}(\sigma_{\alpha}),\  g_{tr} \sim \text{IWP}_2(\sigma_{tr}), \\
        e_i &\overset{iid}{\sim} \mathcal{N}(0,\sigma_e^2).
    \end{aligned}
\end{equation}
Here $y_i$ denote the CO2 concentration observed at the time $x_i$ in years since March 30, 1960. 
The component $g_{tr}$ represents the long term growth trend, modeled using a second order Integrated Wiener process (IWP).
The components $g_1$ and $g_{\frac{1}{2}}$ represent the annual cycle and its first harmonic, modeled using seasonal Gaussian processes (sGP) with one-year and half-year periodicity.
The component $g_{\alpha}$ is another cyclic component, modeled with a sGP with ${\alpha}$-year periodicity, where $\alpha$ is an unknown parameter assigned with an uniform prior between $2$ to $6$. 
All the boundary conditions of the sGP and the IWP are assigned independent priors, $\mathcal{N}(0,1000)$.


## Implement the BOSS algorithm

Let $\alpha$ be the conditioning parameter, the proposed BOSS algorithm is implemented, with $5$ starting values equally spaced in $[2,6]$ and $B = 30$ BO iterations. 

```{r}
lower <- 2; upper <- 6; noise_var <- 1e-6
eval_once <- function(a){
  mod_once <- model_fit(formula = co2 ~ f(x = t1, model = "IWP", order = 2, sd.prior = list(param = 30, h = 10), k = 30, initial_location = median(observed_dataset$t1)) +
                          f(x = t2, model = "sGP", sd.prior = list(param = 1, h = 10), a = (2*pi), m = 2, k = 30) + 
                          f(x = t3, model = "sGP", sd.prior = list(param = 1, h = 10), a = (2*pi/a), m = 1, k = 30),
                        data = observed_dataset,
                        control.family = list(sd.prior = list(param = 1)),
                        family = "Gaussian", aghq_k = 3)
  mod_once$mod$normalized_posterior$lognormconst
}
surrogate <- function(xvalue, data_to_smooth, choice_cov) {
  predict_gp(
    data = data_to_smooth,
    x_pred = matrix(xvalue, ncol = 1),
    choice_cov = choice_cov,
    noise_var = noise_var
  )$mean
}
integrate_aghq <- function(f, k = 100, startingvalue = 0){
  ff <- list(fn = f, gr = function(x) numDeriv::grad(f, x), he = function(x) numDeriv::hessian(f, x))
  aghq::aghq(ff = ff, k = k, startingvalue = startingvalue)$normalized_posterior$lognormconst
}
```


```{r eval=FALSE}
optim.n = 20
begin_time <- Sys.time()
BO_result <- BOSS(eval_once, update_step = 5, max_iter = 30, 
        lower = lower, upper = upper, noise_var = noise_var, initial_design = 5,
        optim.n = optim.n, delta = 0.01,
        criterion = "KL", KL.grid = 3000, KL_check_warmup = 5, KL_iter_check = 5, KL_eps = 0)
end_time <- Sys.time()
end_time - begin_time
save(BO_result, file = paste0(output_path, "/BO_result.rda"))
```


Take a look at the diagnostic plot of the BOSS algorithm:

```{r}
load(paste0(output_path, "/BO_result.rda"))
plot(BO_result$KL_result$KL ~ BO_result$KL_result$i, type = "o", 
     ylab = "KL divergence", xlab = "iteration", cex.lab = 1.5, cex.axis = 1.5)
```


```{r}
data_to_smooth <- list()
data_to_smooth$x <- as.numeric(BO_result$result$x)[order(as.numeric(BO_result$result$x))]
data_to_smooth$x_original <- as.numeric(BO_result$result$x_original)[order(as.numeric(BO_result$result$x))]
data_to_smooth$y <- as.numeric(BO_result$result$y)[order(as.numeric(BO_result$result$x))]
data_to_smooth$y <- data_to_smooth$y - mean(data_to_smooth$y)
```

Take a look at the normalized posterior from the BOSS algorithm:

```{r}
x <- seq(2.01, 5.99, by = 0.01)
y <- qnorm((x - lower)/(upper - lower))
ff <- list()
ff$fn <- function(y) as.numeric(surrogate(pnorm(y), data_to_smooth = data_to_smooth, choice_cov = square_exp_cov_generator_nd(length_scale = BO_result$length_scale, signal_var = BO_result$signal_var)) + dnorm(y, log = TRUE))
fn_vals <- sapply(y, ff$fn)
lognormal_const <- integrate_aghq(f = ff$fn, k = 10)
post_y <- data.frame(y = y, pos = exp(fn_vals - lognormal_const))
post_x <- data.frame(x = pnorm(post_y$y) * (upper - lower) + lower, post = (post_y$pos / dnorm(post_y$y))/(upper - lower) )
plot(post_x$post ~ post_x$x, type = "l", ylab = "Post", 
     xlab = expression(alpha) , cex.lab = 1.5, cex.axis = 1.5)
for(x_val in data_to_smooth$x_original) {
  segments(x_val, -0.02, x_val, -0.05, col = "red")
}
```

Compute the HPD:

```{r}
post_x$x <- round(post_x$x, digits = 2)
my_cdf <- cumsum(post_x$post * c(diff(post_x$x), 0))
my_cdf[which(post_x$x == 3.47)] - my_cdf[which(post_x$x == 3.11)]
plot(post_x$post ~ post_x$x, type = "l", ylab = "Post", 
     xlab = expression(alpha), cex.lab = 2.0, cex.axis = 2.0)
abline(v = 3.47, col = "purple")
abline(v = 3.11, col = "purple")

my_cdf[which(post_x$x == 3.82)] - my_cdf[which(post_x$x == 2.82)]
plot(post_x$post ~ post_x$x, type = "l", ylab = "Post", 
     xlab = expression(alpha), cex.lab = 2.0, cex.axis = 2.0)
abline(v = 3.82, col = "purple")
abline(v = 2.82, col = "purple")
```


To look at the posterior of the latent field (e.g $g$), we can compute the AGHQ rule to integrate out the conditioning parameter $\alpha$.

Let's compute $K = 10$ nodes and weights for the posterior distribution of $\alpha$ using the AGHQ rule.

```{r}
aghq_boss <- function(f, k = 100, data_to_smooth){
  ff <- list(fn = f, gr = function(x) numDeriv::grad(f, x), he = function(x) numDeriv::hessian(f, x))
  opt_result <- list(ff = ff, mode = qnorm(data_to_smooth$x[which.max(data_to_smooth$y)]))
  opt_result$hessian = -matrix(ff$he(opt_result$mode))
  aghq::aghq(ff = ff, k = k, optresults = opt_result, startingvalue = opt_result$mode)
}
aghq_boss_obj <- aghq_boss(f = ff$fn, k = 10, data_to_smooth)

nodes <- aghq_boss_obj[[1]]$grid$nodes
nodes_converted <- as.numeric(pnorm(nodes)*(upper - lower) + lower)
L <- as.numeric(sqrt(aghq_boss_obj$normalized_posterior$grid$features$C))
weights <- as.numeric(aghq_boss_obj[[1]]$nodesandweights$weights)
lambda <- weights * exp(aghq_boss_obj[[1]]$nodesandweights$logpost_normalized)
```


Fit new models at these nodes:

```{r eval=FALSE}
fit_once <- function(a){
  mod_once <- model_fit(formula = co2 ~ f(x = t1, model = "IWP", order = 2, sd.prior = list(param = 30, h = 10), k = 30, initial_location = median(observed_dataset$t1)) +
                          f(x = t2, model = "sGP", sd.prior = list(param = 1, h = 10), a = (2*pi), m = 2, k = 30) + 
                          f(x = t3, model = "sGP", sd.prior = list(param = 1, h = 10), a = (2*pi/a), m = 1, k = 30),
                        data = observed_dataset,
                        control.family = list(sd.prior = list(param = 1)),
                        family = "Gaussian", aghq_k = 3)
  mod_once
}
set.seed(123)
for (i in 1:length(nodes_converted)){
  mod <- fit_once(nodes_converted[i]) 
  save(mod, file = paste0(output_path, "/model", "_", i,".rda"))
}
```


Inferring the latent field:

```{r eval=FALSE}
num_samples <- round(3000 * lambda, 0)
t1_samples <- data.frame(t = seq(0,62, by = 0.01))
t2_samples <- data.frame(t = seq(0,62, by = 0.01))
t3_samples <- data.frame(t = seq(0,62, by = 0.01))

for (i in 1:length(num_samples)) {
  load(paste0(output_path, "/model", "_", i,".rda"))
  pred <- predict(mod, variable = "t1", only.samples = T, newdata = data.frame(t1 = seq(0,62, by = 0.01)))[,-1][,1:num_samples[i]]
  t1_samples <- cbind(t1_samples, pred)
  pred <- predict(mod, variable = "t2", only.samples = T, newdata = data.frame(t2 = seq(0,62, by = 0.01)))[,-1][,1:num_samples[i]]
  t2_samples <- cbind(t2_samples, pred)
  pred <- predict(mod, variable = "t3", only.samples = T, newdata = data.frame(t3 = seq(0,62, by = 0.01)))[,-1][,1:num_samples[i]]
  t3_samples <- cbind(t3_samples, pred)
}
save(t1_samples, file = paste0(output_path, "/t1_samples.rda"))
save(t2_samples, file = paste0(output_path, "/t2_samples.rda"))
save(t3_samples, file = paste0(output_path, "/t3_samples.rda"))
```


Take a look at the results:

```{r}
load(paste0(output_path, "/t1_samples.rda"))
load(paste0(output_path, "/t2_samples.rda"))
load(paste0(output_path, "/t3_samples.rda"))

t_all_samples <- t1_samples + t2_samples + t3_samples
t_all_samples[,1] <- t1_samples[,1]
t_all_summary <- extract_mean_interval_given_samps(t_all_samples)
t_all_summary$time <- (t_all_summary$x * 365.25) + timeOrigin

plot(t_all_summary$q0.5 ~ t_all_summary$time, type = "l", 
     lty = "solid", ylab = "CO2", xlab = "year", cex.lab = 1.5, cex.axis = 1.5)
lines(t_all_summary$q0.975 ~ t_all_summary$time, col = "red", lty = "dashed")
lines(t_all_summary$q0.025 ~ t_all_summary$time, col = "red", lty = "dashed")
```

```{r}
ts_samples <- t2_samples + t3_samples
ts_samples[,1] <- t2_samples[,1]
ts_summary <- extract_mean_interval_given_samps(ts_samples)
ts_summary$time <- (ts_summary$x * 365.25) + timeOrigin

plot(ts_summary$q0.5 ~ ts_summary$time, type = "l", lty = "solid", ylab = "CO2", xlab = "year",
     xlim = as.Date(c("1985-01-01","2000-01-01")), ylim = c(725, 740), cex.lab = 1.5, cex.axis = 1.5)
lines(ts_summary$q0.975 ~ ts_summary$time, col = "red", lty = "dashed")
lines(ts_summary$q0.025 ~ ts_summary$time, col = "red", lty = "dashed")
```



## Implement grid-based algorithm

Finally, to ensure the validity of the BOSS algorithm, we will compare the result with the exact posterior distribution of $\alpha$ obtained through a dense grid-based algorithm.

```{r eval=FALSE}
library(parallel)
x_vals  <- seq(2.01, 6, by = 0.01)
n_cores <- 12
# this returns a list of length length(x_vals)
res_list  <- mclapply(x_vals, eval_once, mc.cores = n_cores)
exact_vals <- unlist(res_list)

# Close the progress bar
exact_grid_result <- data.frame(x = x_vals, exact_vals = exact_vals)
exact_grid_result$exact_vals <- exact_grid_result$exact_vals - max(exact_grid_result$exact_vals)
exact_grid_result$fx <- exp(exact_grid_result$exact_vals)

# Calculate the differences between adjacent x values
dx <- diff(exact_grid_result$x)
# Compute the trapezoidal areas and sum them up
integral_approx <- sum(0.5 * (exact_grid_result$fx[-1] + exact_grid_result$fx[-length(exact_grid_result$fx)]) * dx)
exact_grid_result$pos <- exact_grid_result$fx / integral_approx
save(exact_grid_result, file = paste0(output_path, "/exact_grid_result.rda"))
```

Plot the results:

```{r}
load(paste0(output_path, "/exact_grid_result.rda"))
plot(exact_grid_result$x, exact_grid_result$pos, type = "l", col = "red", xlab = "", ylab = "density", main = "Posterior", lty = 1, cex.lab = 1.5, cex.axis = 1.5)
lines(post_x$x, post_x$post, col = "blue", lty = 2, lwd = 2)
legend("topright", legend = c("BOSS", "Exact"), col = c("blue", "red"), lty = c(2, 1), cex = 1.5)
for(x_val in data_to_smooth$x_original) {
  segments(x_val, -0.02, x_val, -0.05, col = "red")
}
```


As shown in the figure, the BOSS algorithm is able provide a good approximation of the posterior distribution, while requiring significantly fewer evaluations of the likelihood function.




