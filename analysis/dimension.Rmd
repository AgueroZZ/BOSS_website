---
title: "Convergence of BOSS vs Dimension of Parameter Space"
author: "Dayi Li"
date: "2025-04-30"
output: 
  workflowr::wflow_html:
    code_folding: show
editor_options:
  chunk_output_type: console
---


## Introduction

In this example, we analyze how the convergence performance of BOSS changes with the dimension of the conditioning parameter $\boldsymbol{\alpha}$, as well as the choice of the hyperparameter $\delta$.

For simplicity, we set the objective posterior of the conditioning parameter as $d$-dimensional multivariate Gaussian for $d = \{1, 2, 3, 4, 5\}$ with mean vector $\mu_d = \mathbf{0}$ and randomly generated covariance matrices $\Sigma_d$. 

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tikzDevice)

function_path <- "./code"
output_path <- "./output/dimension"
data_path <- "./data/dimension"
source(paste0(function_path, "/00_BOSS.R"))
```

## Generate Random Multivariate Gaussian

```{r}
# objective
eval_func <- function(x, d, Sigma){
  return(mvtnorm::dmvnorm(x, mean = rep(0, d), sigma = Sigma, log = T))
}

d      <- 1:5
sim_id <- 1:20

# pre-allocate result container
res_list <- vector("list", length(d))
for (i in d) 
  res_list[[i]] <- vector("list", length(sim_id))
```

### Run BOSS

```{r eval = F}

for (i in d) {
  for (j in sim_id) {
    success <- FALSE
    attempt <- 0

    while (!success) {
      # each attempt: new seed -> new Sigma
      seed_val <- j + attempt
      set.seed(seed_val)
      
      # regenerate A and Sigma
      A     <- matrix(rnorm(i^2), nrow = i)
      Sigma <- crossprod(A)
      
      # define objective with the new Sigma
      obj_func <- function(x) eval_func(x, d = i, Sigma = Sigma)
      lower    <- rep(-4 * max(Sigma), i)
      upper    <- rep( 4 * max(Sigma), i)
      
      # try BOSS on this Sigma
      out <- try(
        BOSS(obj_func,
             criterion          = "modal",
             update_step        = 5,
             max_iter           = 300,
             D                  = i,
             lower              = lower,
             upper              = upper,
             noise_var          = 1e-6,
             modal_iter_check   = 5,
             modal_check_warmup = 20,
             modal_k.nn         = 5,
             modal_eps          = 0.25,
             initial_design     = 5 * i,
             delta              = 0.01^i,
             optim.n            = 1,
             optim.max.iter     = 100),
        silent = TRUE
      )
      
      if (!inherits(out, "try-error")) {
        # success: save and break out of retry loop
        res_list[[i]][[j]] <- out
        success <- TRUE
      } else {
        message(sprintf(
          "BOSS failed for d=%d, sim=%d (attempt %d, seed=%d). Retrying with new Sigma…",
          i, j, attempt, seed_val
        ))
        attempt <- attempt + 1
      }
    }
  }
}


save(res_list, file = paste0(output_path, "/dimension_test.rda"))
```

```{r}
load(paste0(output_path, "/dimension_test.rda"))

dim <- rep(d, each = 20)
iter <- unlist(lapply(res_list, function(x) lapply(x, function(y) max(y$modal_result$i)))) + 5*dim

iter.data <- data.frame(d= dim, Iteration = iter) %>%
  group_by(d) %>%
  mutate(med = median(Iteration))

ggplot(iter.data, aes(d, Iteration)) + geom_point() + geom_line(aes(d, med))
```

We can see pretty clearly that the number of iterations required for convergence roughly grows exponentially, which is consistent with the existing theoretical analysis of BO convergence.

We also note that the above test only showcases the performance of BOSS when the true posterior is a multivariate Gaussian. 
We have observed that if there is deviation from normality, then convergence could take even longer, especially for higher dimension. 
For example, as the dimension increases, the conditioning parameters could very likely become close to degenerate, which makes the convergence of BOSS extremely difficult.

<!-- ## Changes with Sample Size $n$ -->

<!-- We have seen previously how the number of iterations change with the dimension of the parameter space. We now check how the number of iterations required changes with the sample size $n$. It is important to check this since by Bernstein von-Mises (BvM) theorem, the vast majority of the posterior distribution will converge to a Gaussian as the sample size $n$ increases. This would potentially mean that the posterior mode may be harder to locate. -->

<!-- ```{r} -->
<!-- # objective -->
<!-- eval_func <- function(x, sigma){ -->
<!--   return(dnorm(x, 0, sigma, log = T)) -->
<!-- } -->

<!-- n      <- 10^(0:5) -->
<!-- sim_id <- 1:20 -->

<!-- # pre-allocate result container -->
<!-- res_list <- vector("list", length(n)) -->
<!-- for (i in 1:length(n))  -->
<!--   res_list[[i]] <- vector("list", length(sim_id)) -->
<!-- ``` -->


<!-- ### Run BOSS -->

<!-- ```{r eval = F} -->

<!-- for (i in 1:length(n)) { -->
<!--   for (j in sim_id) { -->
<!--     success <- FALSE -->
<!--     attempt <- 0 -->

<!--     while (!success) { -->
<!--       # each attempt: new seed -> new Sigma -->
<!--       seed_val <- j + attempt -->
<!--       set.seed(seed_val) -->

<!--       # regenerate sigma -->
<!--       sigma <- exp(rnorm(1))/sqrt(n[i]) -->

<!--       # define objective with the new Sigma -->
<!--       obj_func <- function(x) eval_func(x, sigma = sigma) -->
<!--       lower    <- -4 -->
<!--       upper    <- 4 -->

<!--       # try BOSS on this Sigma -->
<!--       out <- try( -->
<!--         BOSS(obj_func, -->
<!--              criterion          = "modal", -->
<!--              update_step        = 5, -->
<!--              max_iter           = 300, -->
<!--              D                  = 1, -->
<!--              lower              = lower, -->
<!--              upper              = upper, -->
<!--              noise_var          = 1e-6, -->
<!--              modal_iter_check   = 5, -->
<!--              modal_check_warmup = 5, -->
<!--              modal_k.nn         = 5, -->
<!--              modal_eps          = 0.25, -->
<!--              initial_design     = 4, -->
<!--              delta              = 0.01, -->
<!--              optim.n            = 1, -->
<!--              optim.max.iter     = 100), -->
<!--         silent = TRUE -->
<!--       ) -->

<!--       if (!inherits(out, "try-error")) { -->
<!--         # success: save and break out of retry loop -->
<!--         res_list[[i]][[j]] <- out -->
<!--         success <- TRUE -->
<!--       } else { -->
<!--         message(sprintf( -->
<!--           "BOSS failed for d=%d, sim=%d (attempt %d, seed=%d). Retrying with new Sigma…", -->
<!--           i, j, attempt, seed_val -->
<!--         )) -->
<!--         attempt <- attempt + 1 -->
<!--       } -->
<!--     } -->
<!--   } -->
<!-- } -->

<!-- save(res_list, file = paste0(output_path, "/sample_size_test.rda")) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- load(paste0(output_path, "/sample_size_test.rda")) -->

<!-- n <- rep(log10(n), each = 20) -->
<!-- iter <- unlist(lapply(res_list, function(x) lapply(x, function(y) max(y$modal_result$i)))) -->

<!-- iter.data <- data.frame(n = n, Iteration = iter) %>% -->
<!--   group_by(n) %>% -->
<!--   mutate(mean = mean(Iteration)) -->

<!-- ggplot(iter.data, aes(n, mean)) + geom_line(aes(n, mean)) + ylab('Average iterations') + xlab('log10(n)') -->
<!-- ``` -->

<!-- We can see that based on 1d Gaussian posterior, it does not seem that the number of iterations changes with the sample size $n$. -->

## Effect of $\delta$

One important hyper-parameter that will also affect convergence is the UCB parameter $\delta$. Previously when we explore the effect of increasing the dimension of $\boldsymbol{\alpha}$, we have set $\delta = 0.01^d$. 
That is, as the dimension increases, we adaptively decrease $\delta$ as a function of $d$. 
This strategy embodies the idea that as the dimension grows, it is exponentially more likely for there to be additional structure in the posterior distribution for BOSS to explore.
As a result, it decreases $\delta$ exponentially to force BOSS to spend more time exploring the entire parameter space. 

When the posterior is close to uni-modal, the influence of $\delta$ is typically small. 
In fact, forcing $\delta$ to be too small may slow down BOSS from finding the posterior mode. 
We here check how would convergence behave if we instead fix $\delta = 0.01$ under the previous multivariate Gaussian example.

```{r}
# objective
eval_func <- function(x, d, Sigma){
  return(mvtnorm::dmvnorm(x, mean = rep(0, d), sigma = Sigma, log = T))
}

d      <- 1:5
sim_id <- 1:20

# pre-allocate result container
res_list <- vector("list", length(d))
for (i in d) 
  res_list[[i]] <- vector("list", length(sim_id))
```

### Run BOSS

```{r eval = F}

for (i in d) {
  for (j in sim_id) {
    success <- FALSE
    attempt <- 0

    while (!success) {
      # each attempt: new seed -> new Sigma
      seed_val <- j + attempt
      set.seed(seed_val)
      
      # regenerate A and Sigma
      A     <- matrix(rnorm(i^2), nrow = i)
      Sigma <- crossprod(A)
      
      # define objective with the new Sigma
      obj_func <- function(x) eval_func(x, d = i, Sigma = Sigma)
      lower    <- rep(-4 * max(Sigma), i)
      upper    <- rep( 4 * max(Sigma), i)
      
      # try BOSS on this Sigma
      out <- try(
        BOSS(obj_func,
             criterion          = "modal",
             update_step        = 5,
             max_iter           = 300,
             D                  = i,
             lower              = lower,
             upper              = upper,
             noise_var          = 1e-6,
             modal_iter_check   = 5,
             modal_check_warmup = 20,
             modal_k.nn         = 5,
             modal_eps          = 0.25,
             initial_design     = 5 * i,
             delta              = 0.01,
             optim.n            = 1,
             optim.max.iter     = 100),
        silent = TRUE
      )
      
      if (!inherits(out, "try-error")) {
        # success: save and break out of retry loop
        res_list[[i]][[j]] <- out
        success <- TRUE
      } else {
        message(sprintf(
          "BOSS failed for d=%d, sim=%d (attempt %d, seed=%d). Retrying with new Sigma…",
          i, j, attempt, seed_val
        ))
        attempt <- attempt + 1
      }
    }
  }
}

save(res_list, file = paste0(output_path, "/delta_test.rda"))
```

```{r}
load(paste0(output_path, "/delta_test.rda"))

dim <- rep(d, each = 20)
iter <- unlist(lapply(res_list, function(x) lapply(x, function(y) max(y$modal_result$i)))) + 5*dim

iter.data <- data.frame(d= dim, Iteration = iter) %>%
  group_by(d) %>%
  mutate(med = median(Iteration))

ggplot(iter.data, aes(d, Iteration)) + geom_point() + geom_line(aes(d, med))
```

Compare the above figure to the previous figure with adaptive $\delta$, we see that the number of iterations until convergence decreases slightly. 
Although the exponential growth with respect to $d$ is still clear.







