---
title: "Simulation 3: Two-dimensional Non-linear Regression"
author: "Dayi Li"
date: "2025-04-19"
output: 
  workflowr::wflow_html:
    code_folding: show
editor_options:
  chunk_output_type: console
---

## Data

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tikzDevice)
library(rstan)
library(INLA)
library(inlabru)
library(modeest)

function_path <- "./code"
output_path <- "./output/sim3"
data_path <- "./data/sim3"
source(paste0(function_path, "/00_BOSS.R"))
```

Consider the following non-linear regression model:
\begin{aligned*}
y_i \mid \log(\rho_i) $\overset{ind}{\sim}\mathcal{N}(\log\rho(r_i), \sigma^2), \\
\log\rho(r_i) = \log\rho_0 - \gamma\log\left\{1 + (r_i/R)^\beta\right\}.
\end{aligned*}

We simulate $n = 200$ data points based on the above model with $\rho_0 = 10$, $R = 2$, $\beta = 2$, $\gamma = -2.5$, and $\sigma = 0.5$. The inferential goal is the nuisance parameters $R$ and $\beta$.

```{r}
r <- seq(0, 20, length.out = 200)
beta <- 10
a <- 2
b <- 2
c <- -2.5

set.seed(1234)
Ir <- beta*(1 + (r/a)^b)^c
lr <- log(Ir) + rnorm(length(r), 0, 0.5)

data <- data.frame(r, lr)

ggplot(data, aes(r, lr)) + geom_point() + ylab('y')
```

## inlabru

We first run `inlabru` to to fit the model. We set the following priors for the parameters:
\begin{aligned*}
\rho_0 \sim \mathcal{N}(0, 1000), \ & R \sim \mathrm{Unif}(0.1, 5), \\
\beta \sim \mathrm{Unif}(0.1, 4),  \ \gamma \sim \mathcal{N}(0, & 1000), \ \sigma^2 \sim \mathrm{Inv-Gamma}(1, 10^{-5}).
\end{aligned*}

```{r}
a_fun <- function(u){
  qunif(pnorm(u), 0.1, 5)
}

b_fun <- function(u){
  qunif(pnorm(u), 0.1, 4)
}

cmp <- ~ a(1, model="linear", mean.linear=0, prec.linear=1) +
  b(1, model="linear", mean.linear=0, prec.linear=1) + 
  c(1) + Intercept(1)

form <- lr ~ Intercept + c*log(1 + (r/a_fun(a))^b_fun(b))

fit <- bru(cmp, formula = form, data = data, family = 'gaussian')
```

## BOSS

Now let's run BOSS. We first specify the (unnormalized) log-posterior for $(R,\beta)$. Note that for this specific problem, the unnormalized log-posterior has a closed-form expression:

```{r}
# specify the objective function for BOSS: unnormalized log posterior of (R, beta)
eval_func <- function(par, x = r, y = lr){
  a <- par[1]
  b <- par[2]
  n <- length(r)
  
  X <- matrix(cbind(rep(1, n), log(1 + (r/a)^b)), ncol = 2)
  Vb <- solve(t(X) %*% X + diag(1/1000, 2))
  P <- diag(n) - X %*% Vb %*% t(X)
  
  mlik <- log(det(Vb))/2 - log(1000) + lgamma((n+1)/2) - (n+1)/2*log(1e-5 + t(y) %*% P %*% y/2) - 
    n/2*log(pi) -5*log(10)
  
  return(mlik)
}

```

Next, we run the BOSS algorithm where the stopping criteria is based on the convergence of the posterior mode. Specifically, we check the modal convergence every $5$ BO iteration, and consider the convergence statistics of the average $5$ nearest neighbor distance around the current mode.

```{r, eval = F}
set.seed(123)
res_opt_modal <- BOSS(eval_func, criterion = 'modal', update_step = 5, max_iter = 100, D = 2,
                        lower = rep(0.1, 2), upper = c(5, 4),
                        noise_var = 1e-6,
                        modal_iter_check = 5,  modal_check_warmup = 20, modal_k.nn = 5,
                        modal_eps = 0.01,
                        initial_design = 5, delta = 0.01^2,
                        optim.n = 5, optim.max.iter = 100)

save(res_opt_modal, file = paste0(output_path, "/BOSS_modal_sim3.rda"))
```

We then run BOSS using AGHQ as convergence statistics. Again, we check for convergence every $5$ iterations. The convergence criteria is relative difference in AGHQ statstics being less than $0.05$.

```{r, eval = F}
set.seed(123)
res_opt_aghq <- BOSS(eval_func, criterion = 'aghq', update_step = 5, max_iter = 100, D = 2,
                      lower = rep(0.1, 2), upper = c(5, 4),
                      noise_var = 1e-6,
                      AGHQ_k = 3, AGHQ_iter_check = 5,  AGHQ_check_warmup = 20, AGHQ_eps = 0.05, buffer = 1e-4,
                      initial_design = 5, delta = 0.01^2,
                      optim.n = 5, optim.max.iter = 100)

save(res_opt_aghq, file = paste0(output_path, "/BOSS_aghq_sim3.rda"))
```

## MCMC

Lastly, we implement the MCMC-based method using `stan` to obtain the oracle. 

```{r, eval = F}
set.seed(1234)
MCMC_fit <- stan(
    file = "code/nlreg.stan",  # Stan program
    data = list(x = r, y = lr, N = length(r)),    # named list of data
    chains = 4,             # number of Markov chains
    warmup = 1000,          # number of warmup iterations per chain
    iter = 20000,            # total number of iterations per chain
    cores = 4,              # number of cores (could use one per chain)
    algorithm = 'NUTS')

# thin the samples fo plotting
MCMC_samp <- as.data.frame(MCMC_fit)
#MCMC_samp_thin <- MCMC_samp[seq(1, 76000, by = 8),]
save(MCMC_samp, file = paste0(output_path, "/MCMC_sim3.rda"))
```


## Results Comparison

We now compare the results of the posterior distributions from `inlabru`, modal-based BOSS, and AGHQ-based BOSS, and MCMC.

### `inlabru` posterior distribution:

```{r, warning = F}
# get joint posterior of (R, beta) from inlabru
joint_samp <- inla.posterior.sample(10000, fit, selection = list(a = 1, b = 1), seed = 12345)
joint_samp <- do.call('rbind', lapply(joint_samp, function(x) matrix(x$latent, ncol = 2)))

inla.joint.samps <- data.frame(a = a_fun(joint_samp[,1]), b = b_fun(joint_samp[,2]))

# plot joint posterior of (R, beta) from inlabru
ggplot(inla.joint.samps, aes(a, b)) + stat_density_2d(
  geom = "raster",
  aes(fill = after_stat(density)), n = 500,
  contour = FALSE) +
  geom_point(data = data.frame(a = a_fun(fit$summary.fixed$mode[1]), b = b_fun(fit$summary.fixed$mode[2])), color = 'red', shape = 1, size =0.5) + 
  geom_point(data = data.frame(a = 2, b = 2), color = 'orange', size =0.5) +
  coord_fixed() + scale_fill_viridis_c(name = 'Density') + theme_minimal() + xlab('$R$') + ylab('$\\beta$') + xlim(c(0.1, 5)) + ylim(c(0.1, 4))
```


### BOSS-modal posterior distribution:

```{r}
load(paste0(output_path, "/BOSS_modal_sim3.rda"))

# get the design points data from BOSS
data_to_smooth <- list()
unique_data <- unique(data.frame(x = res_opt_modal$result$x, y = res_opt_modal$result$y))
data_to_smooth$x <- as.matrix(dplyr::select(unique_data, -y))
data_to_smooth$y <- (unique_data$y - mean(unique_data$y))

square_exp_cov <- square_exp_cov_generator_nd(length_scale = res_opt_modal$length_scale, signal_var = res_opt_modal$signal_var)

surrogate <- function(xvalue, data_to_smooth, cov){
  predict_gp(data_to_smooth, x_pred = xvalue, choice_cov = cov, noise_var = 1e-6)$mean
}

ff <- list()
ff$fn <- function(x) as.numeric(surrogate(x, data_to_smooth = data_to_smooth, cov = square_exp_cov))

x.1 <- (seq(from = 0.1, to = 5, length.out = 300) - 0.1)/4.9
x.2 <- (seq(from = 0.1, to = 4, length.out = 300) - 0.1)/3.9
x_vals <- expand.grid(x.1, x.2)
names(x_vals) <- c('x.1','x.2')
x_original <- t(t(x_vals)*(c(5, 4) - c(0.1, 0.1)) + c(0.1, 0.1)) 

fn_vals <- apply(x_vals, 1, function(x) ff$fn(x = matrix(x, ncol = 2))) + mean(unique_data$y)
# normalize
lognormal_const <- log(sum(exp(fn_vals))*0.0098*0.0078*25/9)
post_x_modal <- data.frame(x_original, pos = exp(fn_vals - lognormal_const))

# plot joint posterior of (R, beta) from BOSS
ggplot(post_x_modal, aes(x.1,x.2)) + geom_raster(aes(fill = (pos))) + 
  geom_point(data = data.frame(x.1 = post_x_modal$x.1[which.max(post_x_modal$pos)], x.2 = post_x_modal$x.2[which.max(post_x_modal$pos)]), color = 'red', shape = 1, size =0.5) +
  geom_point(data = data.frame(x.1 = 2, x.2 = 2), color = 'orange', size =0.5) + coord_fixed() + scale_fill_viridis_c(name = 'Density') + theme_minimal() + xlab('$R$') + ylab('$\\beta$')
```

### BOSS-AGHQ posterior distribuiton:

```{r}
load(paste0(output_path, "/BOSS_aghq_sim3.rda"))

# get the design points data from BOSS
data_to_smooth <- list()
unique_data <- unique(data.frame(x = res_opt_aghq$result$x, y = res_opt_aghq$result$y))
data_to_smooth$x <- as.matrix(dplyr::select(unique_data, -y))
data_to_smooth$y <- (unique_data$y - mean(unique_data$y))

square_exp_cov <- square_exp_cov_generator_nd(length_scale = res_opt_aghq$length_scale, signal_var = res_opt_aghq$signal_var)

surrogate <- function(xvalue, data_to_smooth, cov){
  predict_gp(data_to_smooth, x_pred = xvalue, choice_cov = cov, noise_var = 1e-6)$mean
}

ff <- list()
ff$fn <- function(x) as.numeric(surrogate(x, data_to_smooth = data_to_smooth, cov = square_exp_cov))

x.1 <- (seq(from = 0.1, to = 5, length.out = 300) - 0.1)/4.9
x.2 <- (seq(from = 0.1, to = 4, length.out = 300) - 0.1)/3.9
x_vals <- expand.grid(x.1, x.2)
names(x_vals) <- c('x.1','x.2')
x_original <- t(t(x_vals)*(c(5, 4) - c(0.1, 0.1)) + c(0.1, 0.1)) 

fn_vals <- apply(x_vals, 1, function(x) ff$fn(x = matrix(x, ncol = 2))) + mean(unique_data$y)
# normalize
lognormal_const <- log(sum(exp(fn_vals))*0.0098*0.0078*25/9)
post_x_aghq <- data.frame(x_original, pos = exp(fn_vals - lognormal_const))

# plot joint posterior of (R, beta) from BOSS
ggplot(post_x_aghq, aes(x.1,x.2)) + geom_raster(aes(fill = (pos))) + 
  geom_point(data = data.frame(x.1 = post_x_aghq$x.1[which.max(post_x_aghq$pos)], x.2 = post_x_aghq$x.2[which.max(post_x_aghq$pos)]), color = 'red', shape = 1, size =0.5) +
  geom_point(data = data.frame(x.1 = 2, x.2 = 2), color = 'orange', size =0.5) + coord_fixed() + scale_fill_viridis_c(name = 'Density') + theme_minimal() + xlab('$R$') + ylab('$\\beta$')
```

### MCMC posterior distribution:

```{r, warning=F}
load(paste0(output_path, "/MCMC_sim3.rda"))

ggplot(MCMC_samp, aes(a, b)) + stat_density_2d(
  geom = "raster",
  aes(fill = after_stat(density)), n = 300,
  contour = FALSE) + 
  geom_point(data = data.frame(a = post_x_aghq$x.1[which.max(post_x_aghq$pos)], b = post_x_aghq$x.2[which.max(post_x_aghq$pos)]), color = 'red', shape = 1, size =0.5) +
  geom_point(data = data.frame(a = 2, b = 2), color = 'orange', size =0.5) + coord_fixed() + scale_fill_viridis_c(name = 'Density') + theme_minimal() + xlab('$R$') + ylab('$\\beta$') + xlim(c(0.1, 5)) + ylim(c(0.1, 4))
```


From the above results, it is clear that BOSS is much better at depicting the joint posterior distribution than `inlabru`. The joint distribution from `inlabru` is simply the product of the marginal distribution, which completely ignores the more complex structures in the joint posterior.
